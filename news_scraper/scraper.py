#!/usr/bin/env python3
"""
AI ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ìµœì‹  AI ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•˜ì—¬ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from dateutil import parser as date_parser
import re


class NewsCollector:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, sources_file: str = 'sources.json'):
        """
        Args:
            sources_file: ë‰´ìŠ¤ ì†ŒìŠ¤ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.sources_file = sources_file
        self.sources = self._load_sources()
        self.collected_news = []

    def _load_sources(self) -> List[Dict]:
        """ë‰´ìŠ¤ ì†ŒìŠ¤ ì„¤ì • íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤."""
        try:
            with open(self.sources_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('sources', [])
        except FileNotFoundError:
            print(f"âŒ {self.sources_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        except json.JSONDecodeError:
            print(f"âŒ {self.sources_file} íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return []

    def is_recent(self, pub_date: datetime, hours: int = 24) -> bool:
        """
        ë°œí–‰ì¼ì´ ìµœê·¼ Nì‹œê°„ ì´ë‚´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.

        Args:
            pub_date: ë°œí–‰ì¼ì‹œ
            hours: ê¸°ì¤€ ì‹œê°„ (ê¸°ë³¸ê°’: 24ì‹œê°„)

        Returns:
            ìµœê·¼ ë‰´ìŠ¤ ì—¬ë¶€
        """
        now = datetime.now()
        # pub_dateê°€ timezone-awareì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì²˜ë¦¬
        if pub_date.tzinfo is not None:
            from datetime import timezone
            now = datetime.now(timezone.utc)

        cutoff_time = now - timedelta(hours=hours)
        return pub_date >= cutoff_time

    def collect_from_rss(self, source: Dict) -> List[Dict]:
        """
        RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

        Args:
            source: ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ë³´

        Returns:
            ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        news_list = []

        try:
            print(f"ğŸ“¡ RSS ìˆ˜ì§‘ ì¤‘: {source['name']}...")

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = requests.get(source['url'], headers=headers, timeout=10)
            response.raise_for_status()

            # RSS XML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')

            for item in items:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_tag = item.find('title')
                    if not title_tag:
                        continue
                    title = title_tag.get_text(strip=True)

                    # ë§í¬ ì¶”ì¶œ
                    link_tag = item.find('link')
                    if not link_tag:
                        continue
                    link = link_tag.get_text(strip=True)

                    # ë°œí–‰ì¼ íŒŒì‹±
                    pub_date = None
                    pub_date_tag = item.find('pubDate') or item.find('published') or item.find('dc:date')
                    if pub_date_tag:
                        try:
                            pub_date = date_parser.parse(pub_date_tag.get_text(strip=True))
                        except:
                            pass

                    # ìµœê·¼ 24ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
                    if pub_date and not self.is_recent(pub_date):
                        continue

                    # ìš”ì•½ ì¶”ì¶œ
                    summary = ''
                    desc_tag = item.find('description') or item.find('summary') or item.find('content:encoded')
                    if desc_tag:
                        # HTML íƒœê·¸ ì œê±°
                        desc_soup = BeautifulSoup(desc_tag.get_text(), 'html.parser')
                        summary = desc_soup.get_text(strip=True)[:200]  # 200ìê¹Œì§€ë§Œ

                    news_item = {
                        'source': source['name'],
                        'title': title,
                        'link': link,
                        'summary': summary,
                        'published': pub_date.isoformat() if pub_date else datetime.now().isoformat(),
                        'category': source.get('category', 'unknown')
                    }

                    news_list.append(news_item)

                except Exception as e:
                    print(f"  âš ï¸ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

            print(f"  âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"  âŒ RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

        return news_list

    def collect_from_scraping(self, source: Dict) -> List[Dict]:
        """
        ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

        Args:
            source: ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ë³´

        Returns:
            ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        news_list = []

        try:
            print(f"ğŸŒ ì›¹ ìŠ¤í¬ë˜í•‘ ì¤‘: {source['name']}...")

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = requests.get(source['url'], headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')
            selectors = source.get('selectors', {})

            # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
            articles = soup.select(selectors.get('article', 'article'))

            for article in articles[:10]:  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = article.select_one(selectors.get('title', 'h2'))
                    if not title_elem:
                        continue
                    title = title_elem.get_text(strip=True)

                    # ë§í¬ ì¶”ì¶œ
                    link_elem = article.select_one(selectors.get('link', 'a'))
                    if not link_elem:
                        continue
                    link = link_elem.get('href', '')

                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if link.startswith('/'):
                        from urllib.parse import urljoin
                        link = urljoin(source['url'], link)

                    # ë‚ ì§œ ì¶”ì¶œ (ì„ íƒì )
                    date_elem = article.select_one(selectors.get('date', '.date'))
                    pub_date = datetime.now()
                    if date_elem:
                        try:
                            pub_date = date_parser.parse(date_elem.get_text(strip=True))
                        except:
                            pass

                    news_item = {
                        'source': source['name'],
                        'title': title,
                        'link': link,
                        'summary': '',
                        'published': pub_date.isoformat(),
                        'category': source.get('category', 'unknown')
                    }

                    news_list.append(news_item)

                except Exception as e:
                    print(f"  âš ï¸ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

            print(f"  âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"  âŒ ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")

        return news_list

    def collect_all(self) -> List[Dict]:
        """
        ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

        Returns:
            ìˆ˜ì§‘ëœ ì „ì²´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")

        for source in self.sources:
            if source.get('type') == 'rss':
                news = self.collect_from_rss(source)
                self.collected_news.extend(news)
            elif source.get('type') == 'scraping':
                news = self.collect_from_scraping(source)
                self.collected_news.extend(news)
            print()

        # ë°œí–‰ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        self.collected_news.sort(
            key=lambda x: x.get('published', ''),
            reverse=True
        )

        return self.collected_news

    def save_to_json(self, output_file: str = 'collected_news.json'):
        """
        ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        try:
            data = {
                'collected_at': datetime.now().isoformat(),
                'total_count': len(self.collected_news),
                'news': self.collected_news
            }

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ {output_file}ì— ì €ì¥ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ {len(self.collected_news)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")

        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def print_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        if not self.collected_news:
            print("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print("=" * 60)
        print("ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìš”ì•½")
        print("=" * 60)

        # ì†ŒìŠ¤ë³„ í†µê³„
        source_stats = {}
        for news in self.collected_news:
            source = news['source']
            source_stats[source] = source_stats.get(source, 0) + 1

        for source, count in source_stats.items():
            print(f"  â€¢ {source}: {count}ê°œ")

        print("\n" + "=" * 60)
        print("ìµœì‹  ë‰´ìŠ¤ 5ê°œ:")
        print("=" * 60)

        for i, news in enumerate(self.collected_news[:5], 1):
            print(f"\n{i}. [{news['source']}] {news['title']}")
            print(f"   ğŸ”— {news['link']}")
            if news['summary']:
                print(f"   ğŸ“ {news['summary'][:100]}...")

        print("\n" + "=" * 60 + "\n")


def load_config(config_file: str = 'config.json') -> Optional[Dict]:
    """
    ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        config_file: ì„¤ì • íŒŒì¼ ê²½ë¡œ

    Returns:
        ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸  {config_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í…”ë ˆê·¸ë¨ ì•Œë¦¼ ë¹„í™œì„±í™”)")
        return None
    except json.JSONDecodeError:
        print(f"âŒ {config_file} íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)

    # ì„¤ì • ë¡œë“œ
    config = load_config()

    # ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = NewsCollector()

    # ë‰´ìŠ¤ ìˆ˜ì§‘
    collector.collect_all()

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    collector.print_summary()

    # JSON íŒŒì¼ë¡œ ì €ì¥
    collector.save_to_json()

    # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡
    if config and config.get('telegram', {}).get('enabled', False):
        try:
            from telegram_notifier import TelegramNotifier

            telegram_config = config['telegram']
            bot_token = telegram_config.get('bot_token', '')
            chat_id = telegram_config.get('chat_id', '')

            if bot_token and chat_id and bot_token != 'YOUR_BOT_TOKEN_HERE':
                print("ğŸ“± í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì¤‘...\n")

                notifier = TelegramNotifier(bot_token, chat_id)

                notification_config = config.get('notification', {})
                max_news = notification_config.get('max_news_per_message', 5)
                include_summary = notification_config.get('include_summary', True)

                # ë‰´ìŠ¤ ì•Œë¦¼ ì „ì†¡
                if notifier.send_news_notification(
                    collector.collected_news,
                    format_type='category',
                    max_news=max_news,
                    include_summary=include_summary
                ):
                    print("âœ… í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ!\n")
                else:
                    print("âŒ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨\n")
            else:
                print("âš ï¸  í…”ë ˆê·¸ë¨ ë´‡ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n")

        except ImportError:
            print("âŒ telegram_notifier ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
        except Exception as e:
            print(f"âŒ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì¤‘ ì˜¤ë¥˜: {str(e)}\n")


if __name__ == '__main__':
    main()
